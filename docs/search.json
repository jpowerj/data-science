[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science Without Fear",
    "section": "",
    "text": "Preface\nThis book serves as an introduction to data science, written with the goal of meeting you wherever you’re at in your educational/career journey, regardless of your background in math or computer science. That is to say, we don’t assume any background in STEM, only the desire to learn and the work ethic to put in the hours needed to become comfortable with each topic!\nIt is the companion to the course DSAN5000: Data Science and Analytics, taught at Georgetown University in the Fall of 2023."
  },
  {
    "objectID": "ch01.html#operating-systems",
    "href": "ch01.html#operating-systems",
    "title": "1  Introduction",
    "section": "1.1 Operating Systems",
    "text": "1.1 Operating Systems\n\nLinux:  (Free!)\n\nThe “standard” OS for developers! You can expect instructions to use Linux commands: ls, cd, touch, mv …\n\nWindows:  BUT, proprietary shell (constantly googling “PowerShell equivalent”)\n\n\n\nLinux\nPowerShell\n\n\n\n\nls\nGet-ChildItem\n\n\ncd\nSet-Location\n\n\ntouch\nNew-Item\n\n\nmv\nMove-Item\n\n\n\nOSX:  BUT, OSX and Linux both built on Unix\n\\(\\implies\\) if you know Terminal you know Linux!"
  },
  {
    "objectID": "ch01.html#version-control-with-git-and-github",
    "href": "ch01.html#version-control-with-git-and-github",
    "title": "1  Introduction",
    "section": "1.2 Version Control with Git and GitHub",
    "text": "1.2 Version Control with Git and GitHub\n\n1.2.1 Git vs. GitHub\nDespite the confusingly similar names, it is important to keep in mind the distinction between Git and GitHub!\n\nGit is a command-line program, which on its own just runs on your local computer and keeps track of changes to your code.\nGitHub, on the other hand, is a website which allows you to take your Git repositories and store them online, whether privately or publicly.\n\nThis means, for example, that (if your repository is public) once you push your code to GitHub, others can view it and download it for themselves.\n\n\nGit \n\nCommand-line program\ngit init in shell to create\ngit add to track files\ngit commit to commit changes to tracked files\n\n\nGitHub \n\nCode hosting website\nCreate a repository (repo) for each project\nCan clone repos onto your local machine\n\n\n\n\ngit push/git pull: The link between the two!\n\n\n\n1.2.2 Git Diagram\n\n\n\n1.2.3 Initializing a Repository\nLet’s make a directory for our project called cool-project, and initialize a Git repository for it:\n\nuser@hostname:~$ mkdir cool-project\nuser@hostname:~$ cd cool-project\nuser@hostname:~/cool-project$ git init\nInitialized empty Git repository in /home/user/cool-project/.git/\nThis creates a hidden folder, .git, in the directory:\n\nuser@hostname:~/cool-project$ ls -lah\ntotal 12K\ndrwxr-xr-x  3 user user 4.0K May 28 00:53 .\ndrwxr-xr-x 12 user user 4.0K May 28 00:53 ..\ndrwxr-xr-x  7 user user 4.0K May 28 00:53 .git\n\n\n1.2.4 Adding and Committing a File\nWe’re writing Python code, so let’s create and track cool_code.py:\nuser@hostname:~/cool-project$ touch cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Initial version of cool_code.py\"\n[main (root-commit) b40dc25] Initial version of cool_code.py\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 cool_code.py\n\n\n1.2.5 The Commit Log\nView the commit log using git log:\nuser@hostname:~/cool-project$ git log\ncommit b40dc252a3b7355cc4c28397fefe7911ff3c94b9 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:57:16 2023 +0000\n\n    Initial version of cool_code.py\n\n\n\n\n\ngitGraph\n   commit id: \"b40dc25\"\n\n\n\n\n\n\n\n\n\n\n\n1.2.6 Making Changes\nuser@hostname:~/cool-project$ git status\nOn branch main\nnothing to commit, working tree clean\nuser@hostname:~/cool-project$ echo \"1 + 1\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added code to cool_code.py\"\n[main e3bc497] Added code to cool_code.py\n 1 file changed, 1 insertion(+)\nThe output of the git log command will show the new version:\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12eassets/img/gh_history.png\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial version of cool_code.py\n\n\n\n\n1.2.7 More Changes\nuser@hostname:~/cool-project$ echo \"2 + 2\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\n2 + 2\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Second version of cool_code.py\"\n[main 4007db9] Second version of cool_code.py\n 1 file changed, 1 insertion(+)\nAnd we check the output of git log:\nuser@hostname:~/cool-project$ git log\ncommit 4007db9a031ca134fe09eab840b2bc845366a9c1 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:39:28 2023 +0000\n\n    Second version of cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py\n\n\n1.2.8 Undoing a Commit I\nFirst check the git log to find the hash for the commit you want to revert back to:\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\n\n\n1.2.9 Undoing a Commit II\n\n This is irreversable! \n\nuser@hostname:~/cool-project$ git reset --hard e3bc497ac\nHEAD is now at e3bc497 Added code to cool_code.py\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py\n\n\n1.2.10 Onwards and Upwards\nuser@hostname:~/cool-project$ echo \"3 + 3\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added different code to cool_code.py\"\n[main 700d955] Added different code to cool_code.py\n 1 file changed, 1 insertion(+)\n\nThe final git log output looks like:\nuser@hostname:~/cool-project$ git log\ncommit 700d955faacb27d7b8bc464b9451851b5e319f20 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:44:49 2023 +0000\n\n    Added different code to cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py\n\n\n1.2.11 But Why These Diagrams?\nEven the simplest projects can start to look like:\n\n\n\n\n\ngitGraph\n       commit id: \"537dd67\"\n       commit id: \"6639143\"\n       branch nice_feature\n       checkout nice_feature\n       commit id: \"937ded8\"\n       checkout main\n       commit id: \"9e6679c\"\n       checkout nice_feature\n       branch very_nice_feature\n       checkout very_nice_feature\n       commit id: \"7f4de03\"\n       checkout main\n       commit id: \"6df80c1\"\n       checkout nice_feature\n       commit id: \"bd0ebb8\"\n       checkout main\n       merge nice_feature id: \"9ff61cc\" tag: \"V 1.0.0\" type: HIGHLIGHT\n       checkout very_nice_feature\n       commit id: \"370613b\"\n       checkout main\n       commit id: \"9a07a97\"\n\n\n\n\n\n\n\n\n\n1.2.12 The GitHub Side: Remote\n\n\n\n1.2.13 An Empty Repo\n\n\n\n1.2.14 Refresh after git push\n\n\n\n1.2.15 Commit History\n\n\n\n1.2.16 Checking the diff"
  },
  {
    "objectID": "ch01.html#web-development",
    "href": "ch01.html#web-development",
    "title": "1  Introduction",
    "section": "1.3 Web Development",
    "text": "1.3 Web Development\n\n\n\n\n\n\n\n\n\nFrontend   \nBackend   \n\n\n\n\nLow Level\nHTML/CSS/JavaScript\nGitHub Pages\n\n\nMiddle Level\nJS Libraries\nPHP, SQL\n\n\nHigh Level\nReact, Next.js\nNode.js, Vercel\n\n\n\n\nFrontend icons: UI+UI elements, what the user sees (on the screen), user experience (UX), data visualization Backend icons: Databases, Security\n\n\n1.3.1 Getting Content onto the Internet\n\n\nStep 1: index.html\n\n\nStep 2: Create GitHub repository\n\n\nStep 3: git init, git add -A ., git push\n\n\nStep 4: Enable GitHub Pages in repo settings\n\n\nStep 5: &lt;username&gt;.github.io!\n\n\n\n\n\n\n\n\n\n1.3.2 Deploying from a Branch/Folder"
  },
  {
    "objectID": "ch01.html#the-coding-environment",
    "href": "ch01.html#the-coding-environment",
    "title": "1  Introduction",
    "section": "1.4 The Coding Environment",
    "text": "1.4 The Coding Environment\n\n1.4.1 R vs. RStudio vs. Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUI wrapper around R (Integrated Development Environment = IDE)\nRun blocks of R code (.qmd chunks)\n\n\n\nThe R Language \n\nProgramming language\nRuns scripts via Rscript &lt;script&gt;.r \n\n\n\n\n\n\n+\n\n\n\n\n\n\n\nGUI wrapper around Python (IDE)\nRun blocks of Python code (.ipynb cells)\n\n\n\n\nThe Python Language \n\nScripting language\nOn its own, just runs scripts via python &lt;script&gt;.py\n\n\n\n\n\n\n\n\n\n\n1.4.2 Getting R Onto Your Computer\n\nDownload the R language: https://cran.r-project.org/\n\n\n\nVSCode (recommended)\n\nLinux | Windows | OSX\nInstall the Quarto Extension for VSCode (more info from Quarto here)\n\n\nRStudio\n\nRStudio Desktop\n\n\n\n\n\n1.4.3 Python\n\nThe Python interpreter: Linux | Windows | OSX\nWith VSCode (recommended): you can start coding!\n\n\nmy_python_code.qmd\n\n```{python}\n1 + 1\n```\n\nWith Jupyter there are two options:\n\nJupyterLab (recommended) vs. Jupyter\nMain difference (imo): JupyterLab has a tabbed interface\n\n\n\n\n1.4.4 Google Colab\nAlternative when all else fails! https://colab.research.google.com/\n\nFully web-based Jupyter editing/execution (import/export .ipynb files)\nCan persist code/data to Google Drive\nDrawback: RAM limitations, have to activate GPU mode (pretty much required for any AI work), idle time limitation\n\n\n\n1.4.5 Colab Runtime Options"
  },
  {
    "objectID": "ch02.html#first-things-first-shell-commands",
    "href": "ch02.html#first-things-first-shell-commands",
    "title": "2  Data Science Workflow",
    "section": "2.1 First Things First: Shell Commands",
    "text": "2.1 First Things First: Shell Commands\nOftentimes, given the association of data science with programming languages, your first instinct when starting a data science project will be to create an R or Python file. For the sake of building effective data science habits, however, you should ask yourself if the task you’re trying to accomplish could be done more simply using shell commands. Jurafsky and Martin (2023), for example, one of the most widely-used Natural Language Processing textbooks, introduces the concept of text normalization using Unix commands rather than Python or R. Demonstrating how Unix can quickly (usually far more quickly than R or Python, for simpler tasks like this) process text corpora, they take a text file sh.txt containing the complete works of Shakespeare and use the following Unix command\ntr -sc 'A-Za-z' '\\n' &lt; sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r\nto discover the ten most frequently-occurring words across Shakespeare’s works:\n27378 the\n26084 and\n22538 i\n19771 to\n17481 of\n14725 a\n13826 you\n12489 my\n11318 that\n11112 in\nIn the case of data science in general, simple operations like creating, moving, and copying files, downloading files from the internet, and searching for particular phrases or expressions across your entire disk or within a particular directory are all tasks that can be carried out easily and effectively using a small set of Unix commands."
  },
  {
    "objectID": "ch02.html#r-and-python-for-data-science",
    "href": "ch02.html#r-and-python-for-data-science",
    "title": "2  Data Science Workflow",
    "section": "2.2 R and Python for Data Science",
    "text": "2.2 R and Python for Data Science\nWhen it comes to learning the basics of R, Python, or any other programming language, we believe that there’s no need for us to reinvent the wheel—there are thousands of incredible resources available for free on the internet that can do a better job of teaching the fundamentals of these languages better than we ever could. If you don’t feel comfortable in terms of programming experience, we recommend the following resources in particular (which you can work through and then return to the book!):\n\nDataCamp\nCodecademy\n\nWhether or not you have this background knowledge, don’t worry! We will still work to make sure that we explain new concepts with as little technical jargon as possible throughout."
  },
  {
    "objectID": "ch02.html#python-pandas-and-numpy",
    "href": "ch02.html#python-pandas-and-numpy",
    "title": "2  Data Science Workflow",
    "section": "2.3 Python: Pandas and NumPy",
    "text": "2.3 Python: Pandas and NumPy\nWhile R has a range of libraries that are helpful for reading, manipulating, and analyzing data, in Python two particular libraries stand above the rest: Pandas and NumPy. After installing them on the command-line using\npip install pandas\npip install numpy\nyou’ll be able to import and start using them by including the following at the top of your code:\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\n\nNote that the syntax here, import &lt;x&gt; as &lt;y&gt;, tells Python that you want to import the library whose full name is &lt;x&gt;, but that in your code you’d like to reference this imported library using the shorthand &lt;y&gt;.\n\n\n\n\n\n\nImporting Pandas and NumPy\n\n\n\nIt is standard practice in data science to use this shorthand—pd to refer to pandas and np to refer to numpy—when importing the two libraries. This practice will help immensely when e.g. searching the web for errors in the functions you’re using (since those functions will be written as pd.&lt;function&gt;)\nimport pandas as pd\nimport numpy as np\n\n\nOnce you have imported these libraries, a standard data science workflow for opening a data file and viewing its contents looks like:\n\n\nCode\ndf = pd.read_csv(\"./assets/data/ch02/mydata.csv\")\ndf.head()\n\n\n  index   var_1   var_2   var_3\n0     A  val_A1  val_A2  val_A3\n1     B  val_B1  val_B2  val_B3\n2     C  val_C1  val_C2  val_C3\n\n\nWe can then perform an operation on our data, like dropping the first row, and use head() again to make sure the code did what we expected it to do to our DataFrame:\n\n\nCode\ndf = df.iloc[1:].copy()\ndf.head()\n\n\n  index   var_1   var_2   var_3\n1     B  val_B1  val_B2  val_B3\n2     C  val_C1  val_C2  val_C3\n\n\n\n2.3.1 DataFrames vs. Views of DataFrames\nProbably the most frequent source of errors, over all of the data science couses we’ve taught, has been Pandas’ distinction between a DataFrame itself and a view of a DataFrame. There is a good reason for the distinction, but it is typically a baffling concept for anyone just starting to work with DataFrames. To understand this distinction, we first need to quickly look at the structure of how objects are stored in memory (this is how storage works in general—in Python, R, or any other programming language—so it’s useful knowledge regardless of what particular corner of the data science world you occupy!).\n\n\n2.3.2 The Stack and the Heap\nLet’s look at what happens, in the computer’s memory, when we run the following code:\n\n\nCode\nimport datetime\ncountry_df = pd.read_csv(\"./assets/data/ch02/country_pop.csv\")\npop_col = country_df['pop']\nnum_rows = len(country_df)\nfilled = all(~pd.isna(country_df))\nalg_row = country_df.loc[country_df['name'] == \"Algeria\"]\nnum_cols = len(country_df.columns)\nusername = \"Jeff\"\ncur_date = datetime.datetime.now()\ni = 0\nj = None\nz = 314\n\n\nAnd let’s look at the values Python prints out when we ask it to print some of these variables. First, the whole DataFrame, country_df:\n\n\nCode\ncountry_df\n\n\n      name   pop\n0  Albania   2.8\n1  Algeria  44.2\n2   Angola  34.5\n\n\nNext, pop_col:\n\n\nCode\npop_col\n\n\n0     2.8\n1    44.2\n2    34.5\nName: pop, dtype: float64\n\n\nHow about alg_row?\n\n\nCode\nalg_row\n\n\n      name   pop\n1  Algeria  44.2\n\n\nThese three variables country_df, pop_col, and alg_row may seem like they are three separate, entirely distinct entities in the computer’s memory. After all, we gave them three different names, right? Let’s look at how this situation would be represented in the computer’s memory, however:\n\n\n\n\n\n\ng\n\n\nclusterG0\n\nMemory (After Code Execution)\n\n\nclusterG1\n\nStack\n\n\nclusterG2\n\nHeap\n\n\nclusterG20\n\n\n\n\nstvars\n\ncountry_df\n\npop_col\n\nnum_rows\n\nfilled\n\nalg_row\n\nnum_cols\n\nusername\n\ncur_date\n\ni\n\nj\n\nz\n\n0x03a80afc\n\n0x10ba8\n\n13\n\nTRUE\n\n0xf7fc4380\n\n2\n\n0xf7fc43e0\n\n8ec3d9889\n\n0\n\nNULL\n\n314\n\n\n\nh0\n\nDataFrame object\n\n\n\nstvars:addr0-&gt;h0:d0\n\n\n\n\n\nh1\n\n \n\n0\n\n1\n\n2\n\nname\n\nAlbania\n\nAlgeria\n\nAngola\n\npop\n\n2.8\n\n44.2\n\n34.5\n\n\n\nstvars:addr1-&gt;h1:g5\n\n\n\n\n\nstvars:f4-&gt;h1:g2\n\n\n\n\n\nh3\n\n\"Jeff\"\n\n\n\nstvars:f6-&gt;h3\n\n\n\n\n\nh4\n\nDate[2023-07-22]\n\n\n\nstvars:f7-&gt;h4\n\n\n\n\n\n\n\n\n\nThe Python interpreter did something smart: when we asked it to create a variable called pop_col containing just that column of the full DataFrame, and a variable called alg_row for just one row of the full DataFrame, it knew that it didn’t have to construct two entirely new objects in its memory. Instead, since it knows that country_df contains all of the data that pop_col and alg_row are subsetting, in its box for pop_col it just stored a pointer, pointing to that particular column, rather than an entirely new object built from scratch (in other words, an object for which we’d have to ask the computer to allocate more storage space for us). And similarly, its box for alg_row just contains a pointer to that particular row.\nDisaster strikes, however, if we modify country_df without realizing that this will therefore also modify pop_col and alg_row, since these simply point to specific pieces of country_df.\nA straightforward and effective way to avoid this disaster is by thinking about whether or not you need to persist a particular variable in memory, and whether you’d like to manipulate it separately from the main DataFrame you’re working with. So in our case, if we knew that we wanted to do something to pop_col (say, rescale all the populations to be in units of people rather than millions of people), we could save ourselves a huge headache by writing\n\n\nCode\npop_col = country_df['pop'].copy()\n\n\nNow that we’ve told Python that we want a copy of the column, not just a pointer to a piece of country_df, we can do whatever we want to country_df and/or pop_col without having to worry about whether changes to one will affect the other.\nMost commonly, however, this issue comes about in the following way: when we ask Pandas to perform some operation on our DataFrame, typically we glance at the resulting output from the code, see that it looks correct, and move on. You need to train your brain to intervene when you fall into this habit! Instead, you also need to check that the operation was permanent—i.e., that it was performed in-place rather than as a copy. To see what we mean, let’s try sorting the values in country_df by population, instead of alphabetically:\n\n\nCode\ncountry_df.sort_values(by='pop')\n\n\n      name   pop\n0  Albania   2.8\n2   Angola  34.5\n1  Algeria  44.2\n\n\nAll good, right? No! Stop yourself here! Our brains naturally want to move on once we’ve visually observed the above output: cool, we’re good, I see it and it’s sorted, onto the next task. But the issue is that, by default, Pandas shows us the result of an operation by returning a copy of the original DataFrame with the operation performed, not the original DataFrame itself. Basically, you can think of this like when you make a change to some file and your computer asks you “would you like to save these changes?” By default, Pandas is showing you the changes, and it is your job to SAVE these changes if you want to use the post-operation DataFrame later on in your code. If you don’t, then despite the output of the above code looking correct, the DataFrame will revert to its previous, non-sorted state from the next line of code onwards:\n\n\nCode\ncountry_df\n\n\n      name   pop\n0  Albania   2.8\n1  Algeria  44.2\n2   Angola  34.5\n\n\nLike the .copy() function above, the most straightforward way to avoid this issue is by using the argument that can be passed to most Pandas functions: inplace. As discussed above, the default value is False: the changes are not made to the original DataFrame, but to a copy of it. If you specify inplace=True, however, your problem is solved: this tells Pandas that you’d like the operation to be permanent, so that e.g., using our sort_values() example from above, country_df will now be sorted by population when subsequent lines of code are executed! Let’s look at the difference, examining not the output of sort_values() itself but the state of the DataFrame country_df after sort_values() has been run with the inplace=True argument:\n\n\nCode\ncountry_df.sort_values(by='pop', inplace=True)\ncountry_df\n\n\n      name   pop\n0  Albania   2.8\n2   Angola  34.5\n1  Algeria  44.2\n\n\nSince this is the behavior most students/data scientists intuitively expect anyways, you will eventually get in the habit of adding inplace=True to most of your calls to Pandas functions, but it’s good to take a moment to think about this—for example, imagining some cases where you wouldn’t want Pandas to perform the operation in-place.\n\n\n2.3.3 Manipulating a DataFrame\n\n\nCode\ncountry_df['pop'] = country_df['pop'] * 2\ncountry_df['name'] = country_df['name'].str.lower()\ncountry_df\n\n\n      name   pop\n0  albania   5.6\n2   angola  69.0\n1  algeria  88.4\n\n\nTo understand why these two operations did not require us to use the inplace=True argument we discussed in the previous section, it’s important to note how the = operator works in Python:\n\n\n\n\n\n\nThe Assignment Operator\n\n\n\nOne good thing about R is the fact that it uses the syntax my_var &lt;- value to represent assigning value to the variable my_var. Python, on the other hand, uses the single equals sign, like my_var = value. Thus, when using Python, it’s important to keep in mind the different uses of the equals sign:\n\n= is the assignment operator: it assigns the value on its right-hand side to the variable name on its left hand side\n== is the equality operator: it checks whether the value before it is equal to the value after it, returning True if they’re equal and False otherwise.\n\n\n\nKeep in mind that this assignment operator is evaluated from right to left: when we tell Python e.g. x = y + 3, we are telling it, “first add 3 to the value of the variable y, then store the result of that addition as the variable x.” Therefore, when we write something like x = x + 3, that may be confusing at first glance since x appears on both the left and right sides, keeping this order in mind helps clarify what’s going on: Python first adds 3 to the current value of the variable x, then stores the result of this addition as the variable x (thus overwriting the original value that it used when computing the addition).\nSo, in the case of the two operations we performed on country_df above—multiplying the values in the pop column by 2 and lowercasing all of the values in the name column—we don’t need to specify inplace=True anywhere since we’re explicitly overwriting the values in the columns using the = operator.\nNow, looking at the details of how we performed these operations, note the two ways we’ve now seen to access or update rows and columns of a DataFrame:\n\n\n\n\n\n\nPandas Syntax: Accessing Columns\n\n\n\nIn Pandas, accessing columns is the “default” way to access pieces of a DataFrame, so that all you need to do is specify the name of the DataFrame you want to access a column from, and then include the name of the column surrounded by square brackets:\ndf['col_name']\n\n\n\n\n\n\n\n\nPandas Syntax: Accessing Rows\n\n\n\nUnlike the default column-selection operator, where we just use [] to specify the column we want, accessing rows in Pandas requires us to use a special .loc[] operator, providing a boolean expression within the square brackets which it will use to select only the rows for which the boolean expression evaluates to True. For example, to select only the rows of our country_df DataFrame where the name value is \"Algeria\", we write\ncountry_df.loc[country_df['name'] == \"algeria\"]\nTake a minute to look at how exactly this works, early on, and it will save you lots of headaches in the long run: zooming in on the expression inside the square brackets after .loc, we see\ncountry_df['name'] == \"algeria\"\nThis expression, on its own, goes through each row of country_df and checks if its name value is equal to \"algeria\". If it is, the expression stores a value of True at that row’s index, and False otherwise. Therefore, since the full name column consists of the values [\"albania\",\"algeria\",\"angola\"], the result of the above boolean expression will be [False, False, True]. Our original, full expression:\ncountry_df[country_df['name'] == \"algeria\"]\nwill therefore know (by looking at the [False, False, True] result of the expression inside the square brackets after .loc) to only include the third row, giving us the following result:\n\n\nCode\ncountry_df.loc[country_df['name'] == \"algeria\"]\n\n\n      name   pop\n1  algeria  88.4"
  },
  {
    "objectID": "ch02.html#r-readr-tibbles-and-dplyr",
    "href": "ch02.html#r-readr-tibbles-and-dplyr",
    "title": "2  Data Science Workflow",
    "section": "2.4 R: readr, Tibbles, and dplyr",
    "text": "2.4 R: readr, Tibbles, and dplyr\n\n\n\n\nJurafsky, Daniel, and James H. Martin. 2023. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Youcanprint."
  },
  {
    "objectID": "ch03.html#data-gathering",
    "href": "ch03.html#data-gathering",
    "title": "3  Data Gathering and APIs",
    "section": "3.1 Data Gathering",
    "text": "3.1 Data Gathering\n\nPreexisting data sources\nWeb scraping\nConverting between formats"
  },
  {
    "objectID": "ch03.html#preexisting-data-sources",
    "href": "ch03.html#preexisting-data-sources",
    "title": "3  Data Gathering and APIs",
    "section": "3.2 Preexisting Data Sources",
    "text": "3.2 Preexisting Data Sources\n\nDepending on your field, or the type of data you’re looking for, there may be a “standard” data source!\ne.g., in Economics, for US data, FRED"
  },
  {
    "objectID": "ch03.html#web-scraping",
    "href": "ch03.html#web-scraping",
    "title": "3  Data Gathering and APIs",
    "section": "3.3 Web Scraping",
    "text": "3.3 Web Scraping\n\nFun fact: you can view a webpage’s HTML source code by right-clicking on the page and selecting “View Source”\n\nOn older websites, this means we can just request https://www.page.com and parse the returned HTML\n\nLess fun fact: modern web frameworks like React or Next.js generate pages dynamically using JS, meaning that what you see on the page will not be visible in the HTML source\n\nData scraping still possible for these websites, however, using browser automation tools like Selenium\n\n\n\n3.3.1 Scraping Difficulty\n\n\n\n\n\n\n\n\n\nHow is data loaded?\nSolution\n\n\n\n\nEasy\nData in HTML source\n“View Source”\n\n\nMedium\nData loaded dynamically via API\n“View Source”, find API call, request programmatically\n\n\nHard\nData loaded dynamically via web framework\nUse Selenium"
  },
  {
    "objectID": "ch03.html#data-formats",
    "href": "ch03.html#data-formats",
    "title": "3  Data Gathering and APIs",
    "section": "3.4 Data Formats",
    "text": "3.4 Data Formats\n\nThe most common formats, for most fields1:\n\n.csv: Comma-Separated Values\n.tsv: Tab-Separated Values\n.json: JavaScript Object Notation\n.xls/.xlsx: Excel format\n.dta: Stata format\n\n\n\n3.4.1 Comma/Tab-Delimited: .csv / .tsv\n\n\n👍\n\n\nmydata.csv\n\nindex,var_1,var_2,var_3\nA,val_A1,val_A2,val_A3\nB,val_B1,val_B2,val_B3\nC,val_C1,val_C2,val_C3\nD,val_D1,val_D2,val_D3\n\n\n👎\n\n\nmydata.tsv\n\nindex var_1 var_2 var_3\nA val_A1  val_A2  val_A3\nB val_B1  val_B2  val_B3\nC val_C1  val_C2  val_C3\nD val_D1  val_D2  val_D3\n\n\n\n→\n\n\n\n\nCode\nlibrary(readr)\ndata &lt;- read_csv(\"./assets/data/ch02/mydata.csv\")\n\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): index, var_1, var_2, var_3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ndata\n\n\n# A tibble: 3 × 4\n  index var_1  var_2  var_3 \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 A     val_A1 val_A2 val_A3\n2 B     val_B1 val_B2 val_B3\n3 C     val_C1 val_C2 val_C3\n\n\nCode\n# | index | var_1 | var_2 | var_3 |\n# | - | - | - | - |\n# | A | val_A1 | val_A2 | val_A3 |\n# | B | val_B1 | val_B2 | val_B3 |\n# | C | val_C1 | val_C2 | val_C3 |\n# | D | val_D1 | val_D2 | val_D3 | \n\n\n\n\n\n\nPython: pd.read_csv() (from Pandas library)\nR: read_csv() (from readr library)\n\n\n\n3.4.2 .json\n\n\n\ncourses.json\n\n{\n  \"dsan5000\": {\n    \"title\": \"Data Science and Analytics\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Tools and Workflow\"\n    ]\n  },\n  \"dsan5100\": {\n    \"title\": \"Probabilistic Modeling and Statistical Computing\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Conditional Probability\"\n    ]\n  }\n}\n\n\n\nPython: json (built-in library, import json)\nR: jsonlite (install.packages(jsonlite))\nHelpful validator (for when .json file won’t load)\n\n\n\n3.4.3 Other Formats\n\n.xls/.xlsx: Requires special libraries in Python/R\n\nPython: openpyxl\nR: readxl (part of tidyverse)\n\n.dta: Stata format, but can be read/written to in Python/R\n\nPython: Pandas has built-in pd.read_stata() and pd.to_stata()\nR: read_dta() from Haven library (part of tidyverse)"
  },
  {
    "objectID": "ch03.html#apis",
    "href": "ch03.html#apis",
    "title": "3  Data Gathering and APIs",
    "section": "3.5 APIs",
    "text": "3.5 APIs\n\nApplication Programming Interfaces: the “developer-facing” part of a data pipeline/service\nCan think of it like…\n\nan electrical outlet: you just want electricity from it, without knowing details of Alternating/Direct Currents\nwater fountain: you just want water from it, without knowing details of how it’s pumped into the fountain\ncar: you just want to accelerate, brake, and reverse, without knowing details of combustion engine\n\n\n\n3.5.1 What Does an API Do?\n\nExposes endpoints for use by other developers, without requiring them to know the nuts and bolts of your pipeline/service\nIn our examples…\n\nElectrical outlet: endpoint is socket, wiring not exposed\nWater fountain: endpoint is aerator, pump not exposed\nCar: endpoint is pedals, steering wheel, etc… engine is not exposed\n\n\n\nWhen I’m teaching programming to students in refugee camps who may have never used a computer before, I try to use the idea of “robots”: a program is a robot trained to sit there and wait for inputs, then process them in some way and spit out some output. APIs really capture this notion, honestly.\n\n\n\n3.5.2 Example: Math API\n\nBase URL: https://newton.vercel.app/api/v2/\nThe endpoint: factor\nThe argument: \"x^2 - 1\"\nThe request: https://newton.vercel.app/api/v2/factor/x^2-1\n\n\n\nCode\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nresponse_obj &lt;- req_perform(request_obj)\nwriteLines(response_obj %&gt;% resp_body_string())\n\n\n{\"operation\":\"factor\",\"expression\":\"x^2-1\",\"result\":\"(x - 1) (x + 1)\"}\n\n\n\n\n3.5.3 Math API Endpoints\n\n\n\nOperation\nAPI Endpoint\nResult\n\n\n\n\nSimplify\n/simplify/2^2+2(2)\n8\n\n\nFactor\n/factor/x^2 + 2x\nx (x + 2)\n\n\nDerive\n/derive/x^2+2x\n2 x + 2\n\n\nIntegrate\n/integrate/x^2+2x\n1/3 x^3 + x^2 + C\n\n\nFind 0’s\n/zeroes/x^2+2x\n[-2, 0]\n\n\nFind Tangent\n/tangent/2|x^3\n12 x + -16\n\n\nArea Under Curve\n/area/2:4|x^3\n60\n\n\nCosine\n/cos/pi\n-1\n\n\nSine\n/sin/0\n0\n\n\nTangent\n/tan/0\n0\n\n\n\n\n\n\n3.5.4 Scraping HTML with httr2 and xml2\nhttr2 Documentation | xml2 Documentation\n\n\nCode\n# Get HTML\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://en.wikipedia.org/wiki/Data_science\")\nresponse_obj &lt;- req_perform(request_obj)\n# Parse HTML\nlibrary(xml2)\n\n\n\nAttaching package: 'xml2'\n\n\nThe following object is masked from 'package:httr2':\n\n    url_parse\n\n\nCode\nhtml_obj &lt;- response_obj %&gt;% resp_body_html()\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\n\n{xml_nodeset (5)}\n[1] &lt;span class=\"mw-headline\" id=\"Foundations\"&gt;Foundations&lt;/span&gt;\n[2] &lt;span class=\"mw-headline\" id=\"Etymology\"&gt;Etymology&lt;/span&gt;\n[3] &lt;span class=\"mw-headline\" id=\"Data_Science_And_Data_Analysis\"&gt;Data Scienc ...\n[4] &lt;span class=\"mw-headline\" id=\"See_also\"&gt;See also&lt;/span&gt;\n[5] &lt;span class=\"mw-headline\" id=\"References\"&gt;References&lt;/span&gt;\n\n\n\n\nNote: httr2 is a re-written version of the original httr package, which is now deprecated. You’ll still see lots of code using httr, however, so it’s good to know how both versions work. Click here for a helpful vignette on the original httr library.\n\n\n3.5.5 Navigating HTML with XPath\nXPath Cheatsheet\n\nNotice the last line on the previous slide:\n\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\nThe string passed to xml_find_all() is an XPath selector\n\n\n\nXPath selectors are used by many different libraries, including Selenium (which we’ll look at very soon) and jQuery (a standard extension to plain JavaScript allowing easy searching/manipulation of the DOM), so it’s good to learn it now!\n\n\n3.5.6 XPath I: Selecting Elements\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//div' matches all elements &lt;div&gt; in the document:\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n'//div//img' matches &lt;img&gt; elements which are children of &lt;div&gt; elements:\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n\n\n\n3.5.7 XPath II: Filtering by Attributes\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//p[id=\"page-content\"]' matches all &lt;p&gt; elements with id page-content2:\n&lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\nMatching classes is a bit trickier:\n'//img[contains(concat(\" \", normalize-space(@class), \" \"), \" footer-image \")]'\nmatches all &lt;img&gt; elements with page-content as one of their classes3\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n\n\n\n3.5.8 Authentication\n\nMost APIs don’t allow requests to be made by anonymous requesters, and require authentication.\nFor example, to access private GitHub repos using GitHub’s API, you’ll need to authenticate that you are in fact the one making the request\n\n\n\n3.5.9 Authentication via GH\n\nThe GH library for R can handle this authentication process for you. For example, this private repo in my account does not show up if requested anonymously, but does show up when requested using GH with a Personal Access Token4:\n\n\n\nCode\nlibrary(gh)\n# We use tryCatch so that code still runs even if (a) GITHUB_TOKEN isn't set, or (b) no internet connection\ngh_error &lt;- FALSE\nresult &lt;- tryCatch( \n  {\n    gh(\"GET /repos/jpowerj/private-repo-test\")\n  },\n  error = function(e) {\n    gh_error &lt;- TRUE\n    writeLines(\"Could not fetch github repo\")\n  }\n)\nif (!gh_error) {\n    writeLines(paste0(result$name, \": \",result$description))\n}\n\n\nprivate-repo-test: Private repo example for DSAN5000"
  },
  {
    "objectID": "ch04.html",
    "href": "ch04.html",
    "title": "4  Data Cleaning and Tidy Data",
    "section": "",
    "text": "Data cleaning"
  },
  {
    "objectID": "ch06.html#levels-of-difficulty",
    "href": "ch06.html#levels-of-difficulty",
    "title": "6  Optimization",
    "section": "6.1 Levels of Difficulty",
    "text": "6.1 Levels of Difficulty\n\n\n\nCode\nset.seed(1948)\nknitr::opts_template$set(\n    tikz_settings = list(\n        fig.ext = \"svg\",\n        fig.align = \"center\",\n        engine.opts = list(dvisvgm.opts = \"--font-format=woff\")\n    ),\n    tikz_settings_fancy = list(\n        fig.ext = \"svg\",\n        fig.align = \"center\",\n        engine.opts = list(\n            dvisvgm.opts = \"--font-format=woff\",\n            template = \"./assets/code/tikz2pdf.tex\"\n        )\n    )\n)\ndisp &lt;- function(df, obs_per_page = 6, custom_callback = NULL, ...) {\n    # If length of df is less than obs_per_page, disable pagination\n    dom_str &lt;- \"tp\"\n    if (nrow(df) &lt;= obs_per_page) {\n        dom_str &lt;- \"t\"\n    }\n    return(DT::datatable(\n        df,\n        extensions = c(\"FixedColumns\", \"FixedHeader\"),\n        options = list(\n            pageLength = obs_per_page,\n            scrollX = TRUE,\n            paging = TRUE,\n            dom = dom_str,\n            fixedHeader = TRUE,\n            filter = FALSE,\n            ordering = FALSE,\n            language = list(\n                \"paginate\" = list(\n                    \"previous\" = \"&lt;i class='bi bi-chevron-left'&gt;&lt;/i&gt;\",\n                    \"next\" = \"&lt;i class='bi bi-chevron-right'&gt;&lt;/i&gt;\"\n                )\n            ),\n            callback = custom_callback\n        )\n    ))\n}\n\n# library(knitr)\n## define a method for objects of the class data.frame\n# knit_print.data.frame &lt;- function(x, ...) {\n#    #res &lt;- paste(c(\"\", \"\", disp(x)), collapse = \"\\n\")\n#    asis_output(disp(x))\n# }\n## register the method\n# registerS3method(\"knit_print\", \"data.frame\", knit_print.data.frame)\n\n\n\n\n\n\nCode\n# For slides\nlibrary(ggplot2)\ncbPalette &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\noptions(ggplot2.discrete.colour = cbPalette)\n# Theme generator, for given sizes\ndsan_theme &lt;- function(plot_type = \"full\") {\n    if (plot_type == \"full\") {\n        custom_base_size &lt;- 16\n    } else if (plot_type == \"half\") {\n        custom_base_size &lt;- 22\n    } else if (plot_type == \"quarter\") {\n        custom_base_size &lt;- 28\n    } else {\n        # plot_type == \"col\"\n        custom_base_size &lt;- 22\n    }\n    theme &lt;- theme_classic(base_size = custom_base_size) +\n        theme(\n            plot.title = element_text(hjust = 0.5),\n            plot.subtitle = element_text(hjust = 0.5),\n            legend.title = element_text(hjust = 0.5),\n            legend.box.background = element_rect(colour = \"black\")\n        )\n    return(theme)\n}\n\nknitr::opts_chunk$set(fig.align = \"center\")\ng_pointsize &lt;- 5\ng_linesize &lt;- 1\n# Technically it should always be linewidth\ng_linewidth &lt;- 1\ng_textsize &lt;- 14\n\nremove_legend_title &lt;- function() {\n    return(theme(\n        legend.title = element_blank(),\n        legend.spacing.y = unit(0, \"mm\")\n    ))\n}\n\n\n\n\nEasy: Linear/Quadratic Optimization\nMedium: Convex Optimization\nHard: Nonlinear Optimization\n\n\n\nCode\nlibrary(ggplot2)\nquadFn &lt;- function(x) x^2\nggplot(data=data.frame(x=c(-4,4)), aes(x=x)) +\n  stat_function(fun=quadFn, linewidth = g_linewidth) +\n  dsan_theme(\"quarter\")"
  },
  {
    "objectID": "ch06.html#convex-optimization",
    "href": "ch06.html#convex-optimization",
    "title": "6  Optimization",
    "section": "6.2 Convex Optimization",
    "text": "6.2 Convex Optimization\n\n\n\n\n\n\nConvex Function\n\n\n\nA function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if for all \\(x_1, x_2 \\in \\text{dom} f\\) and \\(0 \\leq \\theta \\leq 1\\) we have\n\\[\n\\color{orange}{f\\left(\\theta x_1 + (1 − \\theta)x_2\\right)} \\leq \\color{lightblue}{\\theta f(x_1) + (1 − \\theta) f(x_2)}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a free, 730-page book containing everything you’ll ever need to know about convex optimization, see: https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf"
  },
  {
    "objectID": "ch06.html#how-does-convexity-help-us",
    "href": "ch06.html#how-does-convexity-help-us",
    "title": "6  Optimization",
    "section": "6.3 How Does Convexity Help Us?",
    "text": "6.3 How Does Convexity Help Us?\n\ntldr: If \\(f\\) is convex, tangent line to \\(f\\) at some point \\(x\\) lies underneath \\(f\\), and points to lower values of \\(f(x)\\)\n\\(\\implies\\) procedure for global minimization via (e.g.) gradient descent"
  },
  {
    "objectID": "ch06.html#loss-functions",
    "href": "ch06.html#loss-functions",
    "title": "6  Optimization",
    "section": "6.4 Loss Functions",
    "text": "6.4 Loss Functions\n\n\nCode\nbase &lt;-\n  ggplot() +\n  xlim(-5, 5) +\n  ylim(0, 25) +\n  labs(\n    x = \"Y[obs] - Y[pred]\",\n    y = \"Prediction Badness (Loss)\"\n  ) +\n  dsan_theme(\"full\")\n\nmy_fn &lt;- function(x) { return(x^2) }\nmy_deriv2 &lt;- function(x) { return(4*x - 4) }\nmy_derivN4 &lt;- function(x) { return(-8*x - 16) }\nbase + geom_function(fun = my_fn, color=cbPalette[1], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=2,y=4)), aes(x=x,y=y), color=cbPalette[2], size=g_pointsize/2) + \n  geom_function(fun = my_deriv2, color=cbPalette[2], linewidth=1) +\n  geom_point(data=as.data.frame(list(x=-4,y=16)), aes(x=x,y=y), color=cbPalette[3], size=g_pointsize/2) + \n  geom_function(fun = my_derivN4, color=cbPalette[3], linewidth=1)"
  },
  {
    "objectID": "ch06.html#why-is-convexity-important",
    "href": "ch06.html#why-is-convexity-important",
    "title": "6  Optimization",
    "section": "6.5 Why is Convexity Important?",
    "text": "6.5 Why is Convexity Important?\n\nNeural networks are trained via an algorithm called backpropagation"
  },
  {
    "objectID": "ch06.html#sec-nn-intro",
    "href": "ch06.html#sec-nn-intro",
    "title": "6  Optimization",
    "section": "6.6 Backing Up: What is a Neural Network?",
    "text": "6.6 Backing Up: What is a Neural Network?\n\nA linked network of \\(L\\) layers each containing nodes \\(\\nu_i^{[\\ell]}\\)\n\n\n\nCode\n\\colorlet{myred}{red!80!black}\n\\colorlet{myblue}{blue!80!black}\n\\colorlet{mygreen}{green!60!black}\n\\colorlet{myorange}{orange!70!red!60!black}\n\\colorlet{mydarkred}{red!30!black}\n\\colorlet{mydarkblue}{blue!40!black}\n\\colorlet{mydarkgreen}{green!30!black}\n\\tikzstyle{node}=[thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6]\n\\tikzstyle{node in}=[node,green!20!black,draw=mygreen!30!black,fill=mygreen!25]\n\\tikzstyle{node hidden}=[node,blue!20!black,draw=myblue!30!black,fill=myblue!20]\n\\tikzstyle{node convol}=[node,orange!20!black,draw=myorange!30!black,fill=myorange!20]\n\\tikzstyle{node out}=[node,red!20!black,draw=myred!30!black,fill=myred!20]\n\\tikzstyle{connect}=[thick,mydarkblue] %,line cap=round\n\\tikzstyle{connect arrow}=[-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten &lt;=0.5,shorten &gt;=1]\n\\tikzset{ % node styles, numbered for easy mapping with \\nstyle\n    node 1/.style={node in},\n    node 2/.style={node hidden},\n    node 3/.style={node out},\n}\n\\def\\nstyle{int(\\lay&lt;\\Nnodlen?min(2,\\lay):3)} % map layer number onto 1, 2, or 3\n\\begin{tikzpicture}[x=2.2cm,y=1.4cm]\n\\readlist\\Nnod{4,5,5,5,3} % array of number of nodes per layer\n\\readlist\\Nstr{n,N_1,N_2,N_3,k} % array of string number of nodes per layer\n\\readlist\\Cstr{\\strut x,\\nu^{[\\prev]},\\nu^{[\\prev]},\\nu^{[\\prev]},y} % array of coefficient symbol per layer\n\\def\\yshift{0.5} % shift last node for dots\n\\foreachitem \\N \\in \\Nnod{ % loop over layers\n  \\def\\lay{\\Ncnt} % alias of index of current layer\n  \\pgfmathsetmacro\\prev{int(\\Ncnt-1)} % number of previous layer\n  \\foreach \\i [evaluate={\\c=int(\\i==\\N); \\y=\\N/2-\\i-\\c*\\yshift;\n    \\index=(\\i&lt;\\N?int(\\i):\"\\Nstr[\\lay]\");\n    \\x=\\lay; \\n=\\nstyle;}] in {1,...,\\N}{ % loop over nodes\n    % NODES\n    \\node[node \\n] (N\\lay-\\i) at (\\x,\\y) {$\\Cstr[\\lay]_{\\index}$};\n    \n    % CONNECTIONS\n    \\ifnum\\lay&gt;1 % connect to previous layer\n    \\foreach \\j in {1,...,\\Nnod[\\prev]}{ % loop over nodes in previous layer\n      \\draw[connect,white,line width=1.2] (N\\prev-\\j) -- (N\\lay-\\i);\n      \\draw[connect] (N\\prev-\\j) -- (N\\lay-\\i);\n      %\\draw[connect] (N\\prev-\\j.0) -- (N\\lay-\\i.180); % connect to left\n    }\n    \\fi % else: nothing to connect first layer\n  }\n  \\path (N\\lay-\\N) --++ (0,1+\\yshift) node[midway,scale=1.5] {$\\vdots$};\n}\n\\end{tikzpicture}"
  },
  {
    "objectID": "ch06.html#what-do-the-nodes-do",
    "href": "ch06.html#what-do-the-nodes-do",
    "title": "6  Optimization",
    "section": "6.7 What Do the Nodes Do?",
    "text": "6.7 What Do the Nodes Do?\nEach node \\(\\nu_i^{[\\ell]}\\) in the network:\n\nTakes in an input,\nTransforms it using a weight \\(w^{[\\ell]}_i\\) and bias \\(b^{[\\ell]}_i\\), and\nProduces an output, typically using a sigmoid function like \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\):\n\n\\[\n\\text{output}^{[\\ell]}_i = \\sigma(w^{[\\ell]}_i \\cdot \\text{input} + b^{[\\ell]}_i)\n\\]"
  },
  {
    "objectID": "ch06.html#how-does-it-learn",
    "href": "ch06.html#how-does-it-learn",
    "title": "6  Optimization",
    "section": "6.8 How Does it “Learn”?",
    "text": "6.8 How Does it “Learn”?\n\nNeed a loss function \\(\\mathcal{L}(\\widehat{y}, y)\\)\nStarting from the end, we backpropagate the loss, updating weights and biases as we go\nHigher loss \\(\\implies\\) greater change to weights and biases"
  },
  {
    "objectID": "ch09.html",
    "href": "ch09.html",
    "title": "9  Neural Networks",
    "section": "",
    "text": "We looked at the basics of neural networks in Section 6.6, but in this chapter we dive head-first into the world of neural networks, which (at least at the time of writing—things change fast in the AI world!) are making waves even outside of the AI community due to popular apps built upon neural networks like ChatGPT and Dall-E."
  },
  {
    "objectID": "ch12.html",
    "href": "ch12.html",
    "title": "12  Image Data",
    "section": "",
    "text": "Given that so much of AI and Machine Learning research has been driven, historically, by the desire to create machines that can emulate the human brain, it makes sense that lots of progress was made early on in creating algorithms for computers to process and reason about visual information—what our brain is doing at every waking moment in its interface with our eyes."
  },
  {
    "objectID": "ch13.html",
    "href": "ch13.html",
    "title": "13  Natural Language Processing",
    "section": "",
    "text": "Data that is “born numeric” is, if you think about it, probably a small portion of all the data that humans have produced over the course of history. When humans started communicating information to one another, it wasn’t in the form of numbers, but of words: natural language, not tabular arrays of numbers, is thus in a sense the “natural” medium for human-relevant information.\nThus in this chapter we delve into the rest of the “data iceberg”, beyond that tip of it in which data arrives in nice, computationally-analyzable numeric form."
  },
  {
    "objectID": "ch14.html",
    "href": "ch14.html",
    "title": "14  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "data-sources.html#general",
    "href": "data-sources.html#general",
    "title": "Appendix A — Data Sources",
    "section": "A.1 General",
    "text": "A.1 General\n\nList of public-facing APIs\nData is Plural: Weekly newsletter with collection of interesting publicly-available datasets\n\nData is Plural dataset archive, as Google spreadsheet"
  },
  {
    "objectID": "data-sources.html#economics",
    "href": "data-sources.html#economics",
    "title": "Appendix A — Data Sources",
    "section": "A.2 Economics",
    "text": "A.2 Economics\n\nFRED: Federal Reserve Economic Data"
  }
]