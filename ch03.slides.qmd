---
title: "Lecture 2: Data Gathering and APIs"
subtitle: "*DSAN 5000: Data Science and Analytics*"
date: last-modified
institute: "<a href=\"mailto:jj1088@georgetown.edu\">`jj1088@georgetown.edu`</a>"
author: "Jeff Jacobs"
categories:
  - "Lecture Slides"
format:
  revealjs:
    cache: false
metadata-files: 
  - "../_revealjs.globals.yml"
---

## What We'll Cover Today

<ul class="menu"></ul>

::: {.hidden}
```{r,echo=FALSE}
set.seed(1948)
knitr::opts_template$set(
    tikz_settings = list(fig.ext = "svg", fig.align = "center", engine.opts = list(dvisvgm.opts = "--font-format=woff"))
  )
```
:::

## Data Gathering {data-name="Data Gathering"}

* Preexisting data sources
* Web scraping
* Converting between formats

## Preexisting Data Sources

* Depending on your field, or the type of data you're looking for, there may be a "standard" data source!
* e.g., in *Economics*, for US data, FRED

## Web Scraping

* Fun fact: you can view a webpage's **HTML source code** by right-clicking on the page and selecting "View Source"
  * On older websites, this means we can just request `https://www.page.com` and parse the returned HTML
* Less fun fact: modern web frameworks like **React** or **Next.js** generate pages dynamically using JS, meaning that what you see on the page will not be visible in the HTML source
  * Data scraping still possible for these websites, however, using browser automation tools like <a href="https://www.selenium.dev/" target="_blank">**Selenium**</a>

## Scraping Difficulty

| | How is data loaded? | Solution |
|-|-|-|
| **Easy** | Data in HTML source | "View Source" |
| **Medium** | Data loaded dynamically via API | "View Source", find API call, request programmatically |
| **Hard** | Data loaded dynamically via web framework | Use <a href="https://www.selenium.dev/" target="_blank">Selenium</a> |

## Data Formats

* The most common formats, for most fields[^dbs]:
  * `.csv`: Comma-Separated Values
  * `.tsv`: Tab-Separated Values
  * `.json`: JavaScript Object Notation
  * `.xls`/`.xlsx`: Excel format
  * `.dta`: Stata format

[^dbs]: This is considering only cases where the data comes in the form of individual **files**. There are also **database** formats like SQL and MongoDB (which we'll learn about later on) where data is spread over many files and obtained via database **queries**.

## `.csv` / `.tsv`

::: columns
::: {.column width="47%"}

üëç

```csv {filename="mydata.csv"}
index,var_1,var_2,var_3
A,val_A1,val_A2,val_A3
B,val_B1,val_B2,val_B3
C,val_C1,val_C2,val_C3
D,val_D1,val_D2,val_D3
```

::: {.small-codeblock}

üëé

```csv {filename="mydata.tsv"}
index var_1 var_2 var_3
A val_A1  val_A2  val_A3
B val_B1  val_B2  val_B3
C val_C1  val_C2  val_C3
D val_D1  val_D2  val_D3
```

:::

:::
::: {.column width="6%"}
&rarr;
:::

::: {.column width="47%"}

::: {.fw-table .small-table .r-stretch}

```{r}
#| label: sample-csv
library(readr)
data <- read_csv("../assets/data/lec02/mydata.csv")
data
# | index | var_1 | var_2 | var_3 |
# | - | - | - | - |
# | A | val_A1 | val_A2 | val_A3 |
# | B | val_B1 | val_B2 | val_B3 |
# | C | val_C1 | val_C2 | val_C3 |
# | D | val_D1 | val_D2 | val_D3 | 
```

:::

:::
:::

* Python: `pd.read_csv()` (from Pandas library)
* R: `read_csv()` (from `readr` library)

## `.json`

::: {.smallish-codeblock}

```json {filename="courses.json"}
{
  "dsan5000": {
    "title": "Data Science and Analytics",
    "credits": 3,
    "lectures": [
      "Intro",
      "Tools and Workflow"
    ]
  },
  "dsan5100": {
    "title": "Probabilistic Modeling and Statistical Computing",
    "credits": 3,
    "lectures": [
      "Intro",
      "Conditional Probability"
    ]
  }
}
```
:::

* Python: <a href="https://docs.python.org/3/library/json.html" target="_blank">`json`</a> (built-in library</a>, `import json`)
* R: <a href="https://cran.r-project.org/web/packages/jsonlite/index.html" target="_blank">`jsonlite`</a> (`install.packages(jsonlite)`)
* <a href="https://jsonlint.com/" target="_blank">Helpful validator</a> (for when `.json` file won't load)

## Other Formats

* `.xls`/`.xlsx`: Requires special libraries in Python/R
  * Python: <a href="https://openpyxl.readthedocs.io" target="_blank">`openpyxl`</a>
  * R: <a href="https://readxl.tidyverse.org/" target="_blank">`readxl`</a> (part of tidyverse)
* `.dta`: Stata format, but can be read/written to in Python/R
  * Python: Pandas has built-in `pd.read_stata()` and `pd.to_stata()`
  * R: `read_dta()` from <a href="https://haven.tidyverse.org/reference/read_dta.html" target="_blank">Haven</a> library (part of tidyverse)

## APIs {data-name="APIs"}

* **A**pplication **P**rogramming **I**nterfaces: the "developer-facing" part of a data pipeline/service
* Can think of it like...
  * an electrical outlet: you just want **electricity** from it, without knowing details of Alternating/Direct Currents
  * water fountain: you just want **water** from it, without knowing details of how it's pumped into the fountain
  * car: you just want to accelerate, brake, and reverse, without knowing details of combustion engine

## What Does an API Do?

* Exposes **endpoints** for use by other developers, without requiring them to know the nuts and bolts of your pipeline/service
* In our examples...
  * Electrical outlet: endpoint is **socket**, wiring not exposed
  * Water fountain: endpoint is **aerator**, pump not exposed
  * Car: endpoint is **pedals**, **steering wheel**, etc... engine is not exposed

::: {.notes}

When I'm teaching programming to students in refugee camps who may have never used a computer before, I try to use the idea of "robots": a program is a robot trained to sit there and wait for inputs, then process them in some way and spit out some output. APIs really capture this notion, honestly.

:::

## Example: Math API

* Base URL: <a href="https://newton.vercel.app/api/v2/" target="_blank">`https://newton.vercel.app/api/v2/`</a>
* The **endpoint**: `factor`
* The **argument**: `"x^2 - 1"`
* The **request**: <a href="https://newton.vercel.app/api/v2/factor/x^2-1" target="_blank">`https://newton.vercel.app/api/v2/factor/x^2-1`</a>
```{r}
#| label: math-api-call
#| echo: true
#| code-fold: show
library(httr2)
request_obj <- request("https://newton.vercel.app/api/v2/factor/x^2-1")
response_obj <- req_perform(request_obj)
writeLines(response_obj %>% resp_body_string())
```

## Math API Endpoints {.smaller}

| Operation | API Endpoint | Result |
| - | - | - |
| Simplify | `/simplify/2^2+2(2)` | `8` |
| Factor | `/factor/x^2 + 2x` | `x (x + 2)` |
| Derive | `/derive/x^2+2x` | `2 x + 2` |
| Integrate | `/integrate/x^2+2x` | `1/3 x^3 + x^2 + C` |
| Find 0's | `/zeroes/x^2+2x` | `[-2, 0]` |
| Find Tangent | `/tangent/2|x^3` | `12 x + -16` |
| Area Under Curve | `/area/2:4|x^3` | `60` |
| Cosine | `/cos/pi` | `-1` |
| Sine | `/sin/0` | `0` |
| Tangent | `/tan/0` | `0` |

<!-- | Inverse Cosine | `/arccos/1` | `0` |
Inverse Sine 	/arcsin/0 	0
Inverse Tangent 	/arctan/0 	0
Absolute Value 	/abs/-1 	1
Logarithm 	/log/2l8 	3 -->

## Scraping HTML with `httr2` and `xml2`

<a href="https://httr2.r-lib.org/" target="_blank">`httr2` Documentation</a> | <a href="https://xml2.r-lib.org/" target="_blank">`xml2` Documentation</a>

```{r}
#| label: httr2-example
#| echo: true
#| code-fold: show
# Get HTML
library(httr2)
request_obj <- request("https://en.wikipedia.org/wiki/Data_science")
response_obj <- req_perform(request_obj)
# Parse HTML
library(xml2)
html_obj <- response_obj %>% resp_body_html()
html_obj %>% xml_find_all('//h2//span[@class="mw-headline"]')
```

::: {.aside}

Note: `httr2` is a re-written version of the original `httr` package, which is now deprecated. You'll still see lots of code using `httr`, however, so it's good to know how both versions work. <a href="https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html" target="_blank">Click here for a helpful vignette</a> on the original `httr` library.

:::

## Navigating HTML with XPath

<a href="https://devhints.io/xpath" target="_blank">XPath Cheatsheet</a>

* Notice the last line on the previous slide:

```r
html_obj %>% xml_find_all('//h2//span[@class="mw-headline"]')
```

* The string passed to `xml_find_all()` is an **XPath selector**

::: {.aside}

XPath selectors are used by many different libraries, including **Selenium** (which we'll look at very soon) and **jQuery** (a standard extension to plain JavaScript allowing easy searching/manipulation of the DOM), so it's good to learn it now!

:::

## XPath I: Selecting Elements

```html {filename="mypage.html"}
<div class="container">
  <h1>Header</h1>
  <p id="page-content">Content</p>
  <img class="footer-image m-5" src="footer.png">
</div>
```

* `'//div'` matches all elements `<div>` in the document:

    ```html
    <div class="container">
      <h1>Header</h1>
      <p id="page-content">Content</p>
      <img class="footer-image m-5" src="footer.png">
    </div>
    ```
* `'//div//img'` matches `<img>` elements which are **children of** `<div>` elements:

    ```html
    <img class="footer-image m-5" src="footer.png">
    ```

## XPath II: Filtering by Attributes {.smaller}



```html {filename="mypage.html"}
<div class="container">
  <h1>Header</h1>
  <p id="page-content">Content</p>
  <img class="footer-image m-5" src="footer.png">
</div>
```

* `'//p[id="page-content"]'` matches all `<p>` elements with id `page-content`[^unique-id]:

    ```html
    <p id="page-content">Content</p>
    ```
* Matching **classes** is a bit trickier:

    [`'//img[contains(concat(" ", normalize-space(@class), " "), " footer-image ")]'`]{.small-codeblock}

    matches all `<img>` elements with `page-content` as one of their classes[^multi-class]

    ```html
    <img class="footer-image m-5" src="footer.png">
    ```

[^unique-id]: In HTML, `id`s are required to be **unique** to particular elements (and elements cannot have more than one `id`), meaning that this should only return a **single** element, for valid HTML code (not followed by all webpages!). Also note the **double-quotes** after `id=`, which are required in XPath.

[^multi-class]: Your intuition may be to just use `'//img[@class="footer-image"]'`. Sadly, however, this will match only elements with `footer-image` as their **only** class. i.e., it will match `<img class="footer-image">` but not `<img class="footer-image another-class">`. This will usually fail, since most elements on modern webpages have several classes. For example, if the site is using <a href="https://getbootstrap.com/docs/5.3/getting-started/introduction/" target="_blank">Bootstrap</a>, `<p class="p-5 m-3"></p>` creates a paragraph element with a padding of 5 pixels and a margin of 3 pixels.

## Authentication

* Most APIs don't allow requests to be made by anonymous requesters, and require **authentication**.
* For example, to access private GitHub repos using GitHub's API, you'll need to authenticate that you are in fact the one making the request

## Authentication via `GH`

* The `GH` library for `R` can handle this authentication process for you. For example, <a href="https://github.com/jpowerj/private-repo-test/" target="_blank">this private repo</a> in my account does not show up if requested anonymously, but does show up when requested using `GH` with a Personal Access Token[^security]:

```{r}
#| label: gh-authenticate
#| echo: true
#| code-fold: show
library(gh)
result <- gh("GET /repos/jpowerj/private-repo-test")
writeLines(paste0(result$name, ": ",result$description))
```

&nbsp;

[^security]: Your code should **never** contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called `GITHUB_TOKEN` containing my Personal Access Token, which `GH` then uses to make authenticated requests.

## Next Week {data-state="hide-menubar"}

* Data Cleaning
* Tidy Data